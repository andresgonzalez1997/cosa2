"""
purina_file_horizontal.py  – versión 2025-05-07
------------------------------------------------
• Limpia tablas horizontales de listas de precio Purina.
• Concatena todas las páginas y añade metadatos (planta, fecha, fuente).
• NUEVO (fix): 
    1. «species» se toma del verdadero rótulo de sección (filas SIN dígitos).
    2. Se corrige el traslape de columnas → siempre 16 cols estándar.
Copiar / pegar y reemplazar el módulo anterior.
"""

from __future__ import annotations
import datetime as _dt
import pathlib
import re
from typing import List, Optional

import pandas as pd
import tabula
from PyPDF2 import PdfReader

# --------------------------------------------------------------------------- #
# 1. Nombres estándar (16 columnas reales)                                    #
# --------------------------------------------------------------------------- #
COLUMN_NAMES: List[str] = [
    "product_number",
    "formula_code",
    "product_name",
    "product_form",
    "unit_weight",
    "pallet_quantity",
    "stocking_status",
    "min_order_quantity",
    "days_lead_time",
    "fob_or_dlv",
    "price_change",
    "list_price",
    "full_pallet_price",
    "half_load_full_pallet_price",
    "full_load_full_pallet_price",
    "full_load_best_price",
]

# --------------------------------------------------------------------------- #
# 2. Columnas numéricas (para evitar errores tipo Parquet/Impala)             #
# --------------------------------------------------------------------------- #
NUMERIC_COLS: List[str] = (
    ["pallet_quantity", "min_order_quantity", "days_lead_time"]
    + COLUMN_NAMES[10:]
)

# --------------------------------------------------------------------------- #
# 3. Regex fecha y planta                                                     #
# --------------------------------------------------------------------------- #
_DATE_PATTERNS = [
    re.compile(r'(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})\s*Effective\s+Date', re.I),
    re.compile(r'Effective\s+Date\s*[-–—]?\s*(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})', re.I),
]


def extract_effective_date(pdf_path: str | pathlib.Path) -> _dt.date:
    reader = PdfReader(str(pdf_path))
    first_page_text = reader.pages[0].extract_text()

    for rx in _DATE_PATTERNS:
        if (m := rx.search(first_page_text)):
            date_str = m.group(1)
            sep = "/" if "/" in date_str else "-"
            mm, dd, yy = date_str.split(sep)
            if len(yy) == 2:
                yy = "20" + yy
            return _dt.datetime.strptime(
                f"{mm}{sep}{dd}{sep}{yy}", f"%m{sep}%d{sep}%Y"
            ).date()

    raise ValueError("No se encontró la fecha efectiva en el PDF.")


def extract_plant_location(pdf_path: str | pathlib.Path) -> str:
    try:
        tables = tabula.read_pdf(
            pdf_path,
            pages=1,
            area=[0, 650, 60, 1000],
            lattice=False,
            guess=False,
            pandas_options={"header": None, "dtype": str},
        )
        if not tables:
            return "PLANTA DESCONOCIDA"

        text = " ".join(tables[0].fillna("").values.flatten())
        if (m := re.search(r"-\s*([A-Za-z &.\-]+?)\s+([A-Za-z]{2})\b", text)):
            city, state = m.groups()
            return f"{city.strip().upper()} {state.upper()}"
        return "PLANTA DESCONOCIDA"
    except Exception:
        return "PLANTA DESCONOCIDA"


# --------------------------------------------------------------------------- #
# 4. Utilidad: leer TODAS las tablas                                          #
# --------------------------------------------------------------------------- #
def _read_tables(pdf_path: str | pathlib.Path):
    try:
        return tabula.read_pdf(
            pdf_path,
            pages="all",
            lattice=True,
            guess=False,
            pandas_options={"dtype": str, "header": None},
        )
    except Exception as exc:
        print("[tabula]", exc)
        return []


# --------------------------------------------------------------------------- #
# 5. Normalización a 16 columnas (corrige traslape hasta 20-ish col.)         #
# --------------------------------------------------------------------------- #
def _standardize(tbl: pd.DataFrame) -> Optional[pd.DataFrame]:
    # Elimina columnas al final que estén completamente vacías
    while tbl.shape[1] > 0 and tbl.iloc[:, -1].isna().all():
        tbl = tbl.iloc[:, :-1]

    # Si sigue siendo menor a 16 → irreparable
    if tbl.shape[1] < 16:
        return None

    # Si es mayor, buscar ventana de 16 que contenga la mayoría de precios numéricos
    if tbl.shape[1] > 16:
        best_start, best_score = 0, -1
        row0 = tbl.iloc[0].astype(str)

        for start in range(tbl.shape[1] - 15):           # inclusive
            slice_ = row0.iloc[start : start + 16]
            # Heurística: cuántas columnas donde valor parece numérico
            score = slice_.str.replace(",", "").str.strip().str.match(r"^-?[\d.]+$").sum()
            if score > best_score:
                best_start, best_score = start, score

        tbl = tbl.iloc[:, best_start : best_start + 16]

    tbl.columns = COLUMN_NAMES
    return tbl


# --------------------------------------------------------------------------- #
# 6. Numeric sanitizer                                                        #
# --------------------------------------------------------------------------- #
def _to_float(val):
    if pd.isna(val):
        return None
    s = str(val).replace(",", "").strip()
    sign = -1 if s.endswith("-") or (s.startswith("(") and s.endswith(")")) else 1
    s = s.strip("()- ")
    try:
        return float(s) * sign
    except ValueError:
        return None


def _fix_numeric(df: pd.DataFrame) -> pd.DataFrame:
    for col in NUMERIC_COLS:
        if col in df.columns:
            df[col] = df[col].apply(_to_float)
    return df


# --------------------------------------------------------------------------- #
# 7. Detección de filas cabecera / fragmentos                                 #
# --------------------------------------------------------------------------- #
HEADER_TOKENS = {
    "PRODUCT", "FORM", "UNIT", "WEIGHT", "PALLET", "MIN", "ORDER", "QUANTITY",
    "DAYS", "LEAD", "TIME", "STOCKING", "STATUS", "FOB", "DLV",
}
_PRICE_RE = re.compile(
    "|".join(
        re.escape(p)
        for p in (
            "PRICE / UNIT", "PRICE IN US DOLLAR", "PRICE IN US DOLLARS",
            "MIN / DAYS", "FORMULA CODE", "MONTHLY", "PAGE",
        )
    ),
    re.I,
)


def _is_header_row(row: pd.Series) -> bool:
    txt = " ".join(row.astype(str)).upper()

    if _PRICE_RE.search(txt):
        return True
    first = str(row.iloc[0]).strip().upper()

    if "FORMULA" in txt and "PRODUCT" in txt:
        return True
    if first.startswith("PRODUCT") and str(row.iloc[1]).upper().startswith("FORMULA"):
        return True
    if pd.isna(row["list_price"]) and any(tok in txt for tok in HEADER_TOKENS):
        return True

    return False


# --------------------------------------------------------------------------- #
# 8. Species extractor (filas sin dígitos)                                    #
# --------------------------------------------------------------------------- #
def _grab_species(tbl: pd.DataFrame):
    """
    Localiza la primera fila donde *ninguna* celda contiene dígitos.
    Retorna (species, tbl_sin_fila_species). Si no encuentra, species=None.
    """
    for idx, row in tbl.iterrows():
        joined = " ".join(row.fillna("").astype(str)).strip()
        if joined and not any(ch.isdigit() for ch in joined):
            species = joined.upper()
            tbl = tbl.drop(idx).reset_index(drop=True)
            return species, tbl
    return None, tbl


# --------------------------------------------------------------------------- #
# 9. MAIN                                                                     #
# --------------------------------------------------------------------------- #
def read_file(pdf_path: str | pathlib.Path) -> pd.DataFrame:
    """
    Procesa el PDF y devuelve un DataFrame con:
        16 columnas estándar + plant_location + date_inserted + source + species
    """
    raw_tables = _read_tables(pdf_path)
    if not raw_tables:
        return pd.DataFrame()

    clean_tables = []
    current_species = None

    for raw_tbl in raw_tables:
        # 1) Species
        current_species, raw_tbl = _grab_species(raw_tbl)
        # Si no detectó en esta tabla, mantiene la anterior (propagación)
        if current_species is None and clean_tables:
            current_species = clean_tables[-1]["species"].iloc[0]

        # 2) Normaliza ancho
        std_tbl = _standardize(raw_tbl)
        if std_tbl is None:
            continue

        # 3) Filtra cabeceras & fragmentos
        std_tbl = std_tbl[~std_tbl.apply(_is_header_row, axis=1)].reset_index(drop=True)
        std_tbl.dropna(how="all", inplace=True)

        if std_tbl.empty:
            continue

        std_tbl["species"] = current_species
        clean_tables.append(std_tbl)

    if not clean_tables:
        return pd.DataFrame()

    df = pd.concat(clean_tables, ignore_index=True)
    df = _fix_numeric(df)

    # 4) Metadatos
    df["plant_location"] = extract_plant_location(pdf_path)
    df["date_inserted"] = extract_effective_date(pdf_path)
    df["source"] = pathlib.Path(pdf_path).name

    # 5) Orden final
    return df[[*COLUMN_NAMES, "plant_location", "date_inserted",
               "source", "species"]]
