Following up on my previous message, I share the following analysis based on recent tests.

Summary of recent attempts

Time filters: I tried limiting the import or refresh to the most recent period (for example, only from 2024), but I found that when running fiscal-year-to-date measures, the model still requires the full detail and that exceeds resources.

Workshop with Diego: We validated whether any other optimizations could apply in Power BI (grouping after generating keys, removing unused columns/relationships, etc.), and the conclusion was the same: the current structure forces processing a lot of detail for margin calculations; even with later step limitations, performance still exceeds Power BI’s resources.

Removed unnecessary objects: I confirmed with internal support that the root cause lies in the inherited architecture; even without my extensions, a model based on massive incremental detail behaves the same way.

Cache impact: Loading such a large historical dataset into Power BI affects the cache load—CUBOVEN + COBO_HIST. In other words, we end up in the same situation as in CQN Ecuador, where refresh is only allowed at certain times due to cache overflows.

Behavior under filters: Basically, when filtering to a single month or a limited range, it responds; but requesting a full fiscal year causes the query to exceed available resources (“Query has exceeded the available resources”).

Cause of the problem

The current model merges the operational table CUBOVEN with the historical COBO_HIST into a very large fact-like table, without intermediate aggregations or effective partitioning. Moreover, dependent tables (like DataBase_SalesDataE2E, DataBase_SalesData, DataBase_CustomerChannel) use the original DataBase_SalesData as source (after all steps are applied), then apply further steps—so any grouping issue or failure in the original cascades to all others.

Annual cumulative measures and rankings must work over the entire history, so Power BI hits memory/capacity limits.

Power BI is designed for analysis over optimized datasets, not for direct ingestion of massive transactional data without a pre-aggregation layer.

There is no properly structured dimensional model with summarized fact tables and dimensions; without this, Power BI has to process detail on every complex query.

Viable options

Redesign of the data layer:
As we do in CQN Ecuador, implement a CDP pipeline that ingests the main tables—CUBOVEN and COBO_HIST—from NIS via Sqoop; then apply in that pipeline all current Power Query transformations to produce an optimized, partitioned table; finally, connect Power BI directly to that processed table without further transformations or excessive data overload.

Segmented report as a temporary patch:

Basically continue filtering by month so the current model can show data.

Document this limitation in the assumptions, indicating that long-range analysis requires refactoring.

Quick to implement, but does not satisfy full historical analysis.

I have spent many hours and effort testing all kinds of optimizations because I truly care about delivering a reliable, useful analysis; however, the limitation stems from the inherited structure of CUBOVEN + COBO_HIST and the lack of a prior aggregation layer—factors that are very difficult to control from within Power BI.
